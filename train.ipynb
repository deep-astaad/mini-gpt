{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50a63003-5412-47ef-9616-d90488608a07",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8bcc5a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mSkipping installation of entry points (`project.scripts`) because this project is not packaged; to install entry points, set `tool.uv.package = true` or define a `build-system`\u001b[0m\n",
      "\u001b[2mResolved \u001b[1m140 packages\u001b[0m \u001b[2min 2ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m110 packages\u001b[0m \u001b[2min 0.11ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv sync --extra cpu\n",
    "## For NVIDIA gpu use !uv sync --extra cu128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "866198fe-82aa-44bc-8118-fe7da3dbeeb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nothing to upgrade\n",
      "\u001b[2mResolved \u001b[1m140 packages\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv tool upgrade --all\n",
    "!uv lock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c881e4c9-f0e6-4714-b7f6-c8f81dff1415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa7fc3d-00e7-45f3-ac9c-b9fb0ab2600b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e739d399-6bc7-47e9-8aa0-b88c353faaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read it in to inspect it\n",
    "with open(\"src/mini_gpt/input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00dfa047-ddfe-46d3-937d-e6eb3739e6b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "193972f4-e949-405d-aeab-11bd9d94d4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's look at the first 1000 characters\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df511e1a-4060-45c7-9efe-4a0d464754fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"\".join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c5efd7-dba7-4de8-8796-3f18d4d8868c",
   "metadata": {},
   "source": [
    "### Apply encoding (convert text into integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d59adfc-5427-4ba3-9c1e-28f6eeba4f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "\n",
    "def encode(s):\n",
    "    return [stoi[c] for c in s]  # encoder: take a string, output a list of integers\n",
    "\n",
    "\n",
    "def decode(s):\n",
    "    return \"\".join(\n",
    "        [itos[i] for i in s]\n",
    "    )  # decoder: take a list of integers, output a string\n",
    "\n",
    "\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8949c23e-6bca-48e6-9d2d-fd397ec40e72",
   "metadata": {},
   "source": [
    "### Use torch and tensors to have faster mathematical calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e28b9b95-5f8e-41a7-af64-e0613890e8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m141 packages\u001b[0m \u001b[2min 11.82s\u001b[0m\u001b[0m                                      \u001b[0m\n",
      "\u001b[2K\u001b[2mAudited \u001b[1m10 packages\u001b[0m \u001b[2min 0.02ms\u001b[0m\u001b[0m                                        \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv add torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a7b7c51-ec6b-43db-b4f2-05e3757469fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
    "import torch  # we use PyTorch: https://pytorch.org\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(\n",
    "    data[:1000]\n",
    ")  # the 1000 characters we looked at earier will to the GPT look like this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0257dd6d-27c0-4ff8-9384-7452378c68c7",
   "metadata": {},
   "source": [
    "### Split data into training and validation dataset to avoid overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f4b64d3e-e255-4346-b003-1a9072d0d659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now split up the data into train and validation sets\n",
    "n = int(0.9 * len(data))  # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8700480c-6747-4ac2-a1f0-84d0fd8ebc08",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Visualize training input and ouput data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6345fdf4-75cb-4119-8671-55b090224972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[: block_size + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42feadc8-27f1-4547-a4ba-a247ff7ae7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target: 47\n",
      "when input is tensor([18, 47]) the target: 56\n",
      "when input is tensor([18, 47, 56]) the target: 57\n",
      "when input is tensor([18, 47, 56, 57]) the target: 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1 : block_size + 1]\n",
    "for t in range(block_size):\n",
    "    context = x[: t + 1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "072b5ca7-a416-4f29-a260-e26dae2bfc26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "----\n",
      "when input is [24] the target: 43\n",
      "when input is [24, 43] the target: 58\n",
      "when input is [24, 43, 58] the target: 5\n",
      "when input is [24, 43, 58, 5] the target: 57\n",
      "when input is [24, 43, 58, 5, 57] the target: 1\n",
      "when input is [24, 43, 58, 5, 57, 1] the target: 46\n",
      "when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
      "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
      "when input is [44] the target: 53\n",
      "when input is [44, 53] the target: 56\n",
      "when input is [44, 53, 56] the target: 1\n",
      "when input is [44, 53, 56, 1] the target: 58\n",
      "when input is [44, 53, 56, 1, 58] the target: 46\n",
      "when input is [44, 53, 56, 1, 58, 46] the target: 39\n",
      "when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
      "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [52] the target: 58\n",
      "when input is [52, 58] the target: 1\n",
      "when input is [52, 58, 1] the target: 58\n",
      "when input is [52, 58, 1, 58] the target: 46\n",
      "when input is [52, 58, 1, 58, 46] the target: 39\n",
      "when input is [52, 58, 1, 58, 46, 39] the target: 58\n",
      "when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
      "when input is [25] the target: 17\n",
      "when input is [25, 17] the target: 27\n",
      "when input is [25, 17, 27] the target: 10\n",
      "when input is [25, 17, 27, 10] the target: 0\n",
      "when input is [25, 17, 27, 10, 0] the target: 21\n",
      "when input is [25, 17, 27, 10, 0, 21] the target: 1\n",
      "when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
      "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4  # how many independent sequences will we process in parallel?\n",
    "block_size = 8  # what is the maximum context length for predictions?\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i : i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1 : i + block_size + 1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "\n",
    "xb, yb = get_batch(\"train\")\n",
    "print(\"inputs:\")\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print(\"targets:\")\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print(\"----\")\n",
    "\n",
    "for b in range(batch_size):  # batch dimension\n",
    "    for t in range(block_size):  # time dimension\n",
    "        context = xb[b, : t + 1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9c5dcf49-f426-4ed3-bcf5-9e87c4975703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n"
     ]
    }
   ],
   "source": [
    "print(xb)  # our input to the transformer\n",
    "print(yb)  # our target for given input to the transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778b2c82-4522-4d41-b3c4-834adf48b392",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Prepare basic Bigram language model to calculate loss and logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac9a136c-4df0-4762-b9ae-76337259ca03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f5b5d35f490>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ecbc25db-6ff3-48d1-8117-ceaff6811663",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple Bigram Language Model.\n",
    "    When given a character, it predicts the character that is most likely to come next.\n",
    "    It doesn't have any memory of characters before the immediately preceding one.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocabulary_size):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "            This is the heart of our model: an Embedding table.\n",
    "            It has one row for each character in our vocabulary.\n",
    "            Each row contains the \"logits\" (raw scores) for the *next* character in the sequence.\n",
    "            So, row `i` of this table contains the model's prediction for what comes after character `i`.\n",
    "            The size is (vocabulary_size, vocabulary_size) because for each character, we need to output\n",
    "            a score for every possible next character.\n",
    "        \"\"\"\n",
    "        self.token_embedding_table = nn.Embedding(vocabulary_size, vocabulary_size)\n",
    "\n",
    "    def forward(self, input_indices, targets=None):\n",
    "        \"\"\"\n",
    "        This is the \"forward pass\". It's how the model makes predictions and calculates its error.\n",
    "        - input_indices: A (Batch, Time) tensor of character indices.\n",
    "        - targets: A (Batch, Time) tensor of the true next character indices.\n",
    "        \"\"\"\n",
    "        # For every input character index, we look up its corresponding row in the embedding table.\n",
    "        # This row contains the raw prediction scores (logits) for the next character.\n",
    "        # Input shape: (Batch, Time) -> (B, T)\n",
    "        # Output shape: (Batch, Time, Channel/VocabSize) -> (B, T, C)\n",
    "        logits = self.token_embedding_table(input_indices)\n",
    "\n",
    "        # The loss is a measure of how wrong our model's predictions (logits) were\n",
    "        # compared to the actual targets. We want to minimize this value.\n",
    "        if targets is None:\n",
    "            # If we're just generating text, we don't have targets, so there's no loss.\n",
    "            loss = None\n",
    "        else:\n",
    "            # To calculate the loss, PyTorch's cross_entropy function needs our tensors\n",
    "            # in a specific shape. We flatten the Batch and Time dimensions into one.\n",
    "            # Logits shape: (B, T, C) -> (B*T, C)\n",
    "            # Targets shape: (B, T) -> (B*T)\n",
    "            B, T, C = logits.shape\n",
    "            logits_reshaped = logits.view(B * T, C)\n",
    "            targets_reshaped = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits_reshaped, targets_reshaped)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, starting_indices, max_new_tokens):\n",
    "        \"\"\"\n",
    "        This function generates new text, token by token.\n",
    "        - starting_indices: The starting sequence of characters, e.g., a single newline. (B, T) tensor.\n",
    "        - max_new_tokens: The maximum number of new tokens (characters) to generate.\n",
    "        \"\"\"\n",
    "        current_indices = starting_indices\n",
    "        # Loop for the number of tokens we want to generate\n",
    "        for _ in range(max_new_tokens):\n",
    "            # 1. Get Predictions (Forward Pass)\n",
    "            # We get the logits from the model for the current sequence of indices.\n",
    "            # We don't need the loss here since we are not training.\n",
    "            logits, loss = self.forward(current_indices)\n",
    "\n",
    "            # 2. Focus on the Last Prediction\n",
    "            # The logits tensor contains predictions for *every* timestep. We only care about\n",
    "            # the prediction for the very last character in our sequence.\n",
    "            # So, we grab the logits from the last time step.\n",
    "            # Shape: (B, T, C) -> (B, C)\n",
    "            last_step_logits = logits[:, -1, :]\n",
    "\n",
    "            # 3. Convert Logits to Probabilities\n",
    "            # Softmax turns our raw scores (logits) into a probability distribution.\n",
    "            # All probabilities will sum to 1.\n",
    "            # Shape: (B, C)\n",
    "            probs = F.softmax(last_step_logits, dim=-1)\n",
    "\n",
    "            # 4. Sample a New Character\n",
    "            # We sample from the probability distribution to pick the next character.\n",
    "            # This adds randomness. If we always picked the highest probability, the model\n",
    "            # would be very repetitive.\n",
    "            # Shape: (B, 1)\n",
    "            next_index = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "            # 5. Append the New Character\n",
    "            # We add our newly chosen character's index to the end of our running sequence.\n",
    "            # This new sequence becomes the input for the next iteration of the loop.\n",
    "            # Shape: (B, T+1)\n",
    "            current_indices = torch.cat((current_indices, next_index), dim=1)\n",
    "\n",
    "        return current_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "573aa419-c089-4adb-b6e6-c6360181db03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Forward Pass and Loss Calculation ---\n",
      "Shape of Logits: torch.Size([4, 8, 65])\n",
      "Loss (how wrong the model is): 4.878634929656982\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of our model\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "\n",
    "# Perform a forward pass with our batch of data\n",
    "print(\"\\n--- Forward Pass and Loss Calculation ---\")\n",
    "logits, loss = model.forward(xb, yb)\n",
    "print(\"Shape of Logits:\", logits.shape)\n",
    "print(\n",
    "    \"Loss (how wrong the model is):\", loss.item()\n",
    ")  # .item() gets the raw number from the tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b9cbc371-1dc9-402a-be90-a27a881b99d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating New Text ---\n",
      "Generated text:\n",
      " \n",
      "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHj\n"
     ]
    }
   ],
   "source": [
    "# Let's generate some text!\n",
    "# We'll start with a single token, a tensor containing [[0]], which is a placeholder.\n",
    "# In a real scenario, this might be the index for a newline character.\n",
    "print(\"\\n--- Generating New Text ---\")\n",
    "start_context = torch.zeros((1, 1), dtype=torch.long)\n",
    "generated_indices = model.generate(starting_indices=start_context, max_new_tokens=50)[0]\n",
    "generated_text = decode(generated_indices.tolist())\n",
    "\n",
    "print(\"Generated text:\\n\", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de636d8c-e0e3-4aaf-a52e-b5e275eea19f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Train Bigram model on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "773a5f2b-2311-49b0-b3f6-e1291efab9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "# Its one and only job is to update the model's internal numbers (parameters)  -- in our case, the numbers inside the token_embedding_table,\n",
    "# to make the model better.\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7c3903a3-6c1d-4072-8e94-dde63c1088c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.716907501220703\n"
     ]
    }
   ],
   "source": [
    "# The training loop. We will repeat the learning process 100 times.\n",
    "for steps in range(100):\n",
    "    # --- Step 1: Get a Practice Worksheet ---\n",
    "    # We get a small, random sample of data (a \"batch\") from our training set.\n",
    "    # xb = the practice problems (e.g., \"hello worl\")\n",
    "    # yb = the correct answers (e.g., \"ello world\")\n",
    "    xb, yb = get_batch(\"train\")\n",
    "\n",
    "    # --- Step 2: Take the Test (The \"Forward Pass\") ---\n",
    "    # The model (student) looks at the problems (xb) and makes its best guess\n",
    "    # at the answers. The results are the raw scores (logits) and the grade (loss).\n",
    "    # The 'loss' is a single number telling us how wrong the model was on this batch.\n",
    "    logits, loss = model(xb, yb)\n",
    "\n",
    "    # --- Step 3: Prepare for Feedback (Erase Old Notes) ---\n",
    "    # This is a critical housekeeping step. We tell the teacher (optimizer) to\n",
    "    # forget about the grades from the *previous* practice test. If we don't do this,\n",
    "    # the feedback from all past tests would get jumbled up.\n",
    "    # `set_to_none=True` is just a small performance optimization.\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    # --- Step 4: Figure Out What Went Wrong (The \"Backward Pass\") ---\n",
    "    # This is where the magic happens. The grade (loss) is used to calculate\n",
    "    # exactly how each individual parameter in the model contributed to the final\n",
    "    # error. This process is called backpropagation.\n",
    "    # It gives the teacher a \"report card\" (called gradients) that says, \"this\n",
    "    # parameter was very wrong and should be decreased,\" or \"this one was pretty\n",
    "    # good and should be increased slightly.\"\n",
    "    loss.backward()\n",
    "\n",
    "    # --- Step 5: Update the Knowledge (Apply Corrections) ---\n",
    "    # The teacher (optimizer) takes the report card (gradients) from the\n",
    "    # backward pass and uses it to update the model's parameters.\n",
    "    # It nudges every number in the model in the correct direction, according\n",
    "    # to the learning rate (lr) we set earlier.\n",
    "    # This is the moment the model *actually learns and gets smarter*.\n",
    "    optimizer.step()\n",
    "\n",
    "# After the loop finishes...\n",
    "# We print the grade (loss) from the VERY LAST practice test the model took.\n",
    "# We hope that after 100 rounds of practice, this final grade is much better\n",
    "# (a much lower number) than it was at the beginning.\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8b5c2089-4e9a-4ccf-8ded-5493b5f9b983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating New Text (After training model) ---\n",
      "Generated text:\n",
      " \n",
      "ogM$J:ChFN&Bju3fVb\n",
      "CUzkzepxEG3SPttRF.yCX$pxBME,NBjKpNSTFviMuBUCpXRr'dxrIcL&ya N:VO-FbPlcGonDq\n",
      "nN3:CEQJnA:AtepgH?BofY.R:3f?:BuFjPDnlIotRwPi.B-tRju3faevrIk!bHWCJJm,ZRFVOl,stteAUv''V;zL:CERyCYhssC'nFbotv mN3f fHx'O3!eNc'jbHxrBjmn?YCjuYUg:ChVhchOlyGJMnk$qbbI$-xKuCajXa!UXNeNT'XQ!wtNCIgcdZiNXGQzkHEJpEr?E:JGMAUNH!aHQFsmZvjnQ:'PUzPmrFM\n",
      "jlcir'VJK&D-OKvCUim:GXOBLvya!b\n",
      "rIXIdZ:CKHd?$gRCONgcfmrX:f?RFEaW:CdDM$GG;RltehsZya!PTS.Db?3fhs!IqNUm;Jgbkzeh.:CjgX?VwbfSF$I''VbHEn$slyKd-msBkLI&e?ay,eEX3Hd,KNMjpmO;aeASiZO\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Generating New Text (After training model) ---\")\n",
    "start_context = torch.zeros((1, 1), dtype=torch.long)\n",
    "generated_indices = model.generate(starting_indices=start_context, max_new_tokens=500)[\n",
    "    0\n",
    "]\n",
    "generated_text = decode(generated_indices.tolist())\n",
    "\n",
    "print(\"Generated text:\\n\", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242ac91e-7596-4f0a-abdb-aea793d522d3",
   "metadata": {},
   "source": [
    "## The mathematical trick in self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da745a7-dc42-46da-92ce-c2c1d6a34f40",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Imagine you have a sequence of items (like words in a sentence). \\\n",
    "For each item, you want to create a new representation of it by averaging it with the items that came before it. \n",
    "1. The 1st new item is just the 1st old item.\n",
    "2. The 2nd new item is an average of the 1st and 2nd old items.\n",
    "3. The 3rd new item is an average of the 1st, 2nd, and 3rd old items.\n",
    "4. And so on..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf578e6-2382-439c-b476-cb0984abc37b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Matrix multiplication for weighted aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "571430e4-12ae-42ab-80f4-db9c5bf077c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing matrix A:\n",
      " tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "print(\"Printing matrix A:\\n\", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "160b1b6e-70c1-4340-a693-da99fffa7a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing matrix A:\n",
      " tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n"
     ]
    }
   ],
   "source": [
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "print(\"Printing matrix A:\\n\", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "761ca0c0-71ee-4b73-950d-e0b0fca9604e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing matrix B:\n",
      " tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n"
     ]
    }
   ],
   "source": [
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "print(\"Printing matrix B:\\n\", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "418eedd4-27fd-4e3d-842c-3947eebc3f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing matrix C:\n",
      " tensor([[ 2.,  7.],\n",
      "        [ 8., 11.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "c = a @ b\n",
    "print(\"Printing matrix C:\\n\", c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806327de-174d-4d0c-ab31-f0ba8bcc4d44",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Different implementation of self-attention (aggregated sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "bf4ca903-a129-4e04-9b9a-240fabaa7796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# consider the following toy example:\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 2  # batch, time, channels\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577b379e-4707-4110-b11e-3d78c5a22a23",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Version 1.\n",
    "Using custom loop to add new character and average it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c6511aa7-217b-46e4-9ad3-9acc67890832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
    "xbow = torch.zeros((B, T, C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, : t + 1]  # (t,C)\n",
    "        xbow[b, t] = torch.mean(xprev, 0)\n",
    "\n",
    "xbow[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844b7fa4-c8df-4b0c-9fe5-832fb5e64093",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Version 2\n",
    "Using matrix multiplication (discussed in previous block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d21656dd-e89e-4c0f-bf9c-372e22278561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 2: using matrix multiply for a weighted aggregation\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "xbow2 = wei @ x  # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
    "\n",
    "xbow2[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f81eed1-7c1d-4891-8534-dbc728e662be",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Version 3\n",
    "Using Softmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0b8372c6-c351-4b21-aacd-f3d5eff362fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 3: use Softmax\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril == 0, float(\"-inf\"))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow3 = wei @ x\n",
    "xbow3[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c99fab-3507-451c-be53-85648a0edb8c",
   "metadata": {},
   "source": [
    "## Self-attention head\n",
    "give each word (or \"token\") three different roles:\n",
    "1. Query (q): When a token wants to find relevant information, it sends out a \"query.\" This is like you holding up a sign saying, \"I am writing about topic A, who has information about this?\"\n",
    "2. Key (k): Each token in the sequence has a \"key.\" This is like the title or index card of a book, saying, \"I contain information about topic A.\"\n",
    "3. Value (v): This is the actual content or information of the token. It's the answer you get when you find a match. \"Here is the information I have on topic A.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8afea41a-7852-41ac-82aa-34286e447e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 32  # batch, time, channels\n",
    "x = torch.randn(B, T, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fdca8d84-a57b-4377-b714-d66bb533c00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see a single Head perform self-attention\n",
    "head_size = 16\n",
    "# A linear layer to produce the \"Key\" for each token.\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "# A linear layer to produce the \"Query\" for each token.\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "# A linear layer to produce the \"Value\" for each token.\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "# Now, let's generate the K, Q, and V vectors for every token in our input x\n",
    "k = key(x)  # (B, T, 16) - Each of the T tokens now has a Key vector of size 16.\n",
    "q = query(x)  # (B, T, 16) - Each of the T tokens now has a Query vector of size 16."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d63ecc-73ac-4abc-b68c-a31cffe907e8",
   "metadata": {},
   "source": [
    "### The Calculation: Step-by-Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc6a8a7-8154-40a6-addb-0c14598acfa5",
   "metadata": {},
   "source": [
    "#### Step 1: Find Affinity Scores (Dot Product)\n",
    "Q: Why do a dot product? \\\n",
    "A: If a query vector and a key vector are pointing in a similar direction, their dot product will be high. \\\n",
    "The resulting wei (weights) matrix is a T x T grid of scores. The value at wei[i, j] tells us how much attention token i should pay to token j."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a7fa7884-1397-4c16-bbd8-d8ccd9d2215c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match every query with every key.\n",
    "# q shape: (B, T, 16)\n",
    "# k shape: (B, T, 16) -> we transpose the last two dimensions to (B, 16, T)\n",
    "# The @ performs a batch matrix multiplication.\n",
    "# Resulting shape: (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "wei = q @ k.transpose(-2, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ac5324-9eb8-4396-ba6a-5b105854a2a9",
   "metadata": {},
   "source": [
    "#### Step 2: Masking (Preventing Cheating)\n",
    "Q: What is masking? \\\n",
    "A: We can't let a token \"see\" into the future. Token 3 should only be able to get information from tokens 1, 2, and 3, not from token 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "18638e7b-fae8-44fa-baac-b1dab81818d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a lower-triangular matrix of ones.\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "# Where tril is 0 (the upper triangle), replace the values in 'wei' with negative infinity.\n",
    "wei = wei.masked_fill(tril == 0, float(\"-inf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9702279-4f26-470f-aba1-afea6dd3b5cc",
   "metadata": {},
   "source": [
    "#### Step 3: Softmax (Normalize Scores into Weights)\n",
    "We use the softmax function to turn them into nice percentages that all add up to 1. When softmax sees -inf, it turns it into 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "2a044c41-5a16-4097-b2ab-4260e3e26356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax turns scores into a probability distribution (weights).\n",
    "# dim=-1 means the softmax is applied across each row.\n",
    "wei = F.softmax(wei, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8292f8a5-abc1-4998-bca4-438a0c77554c",
   "metadata": {},
   "source": [
    "#### Step 4: Aggregate the Values\n",
    "We have the attention weights, we can finally create our new, improved token representations. \\\n",
    "We do this by performing a weighted sum of all the Value vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "74c3f1c4-5dd8-44cd-b6c2-e6016449c50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the \"Value\" for each token.\n",
    "v = value(x)\n",
    "# Perform the weighted aggregation.\n",
    "out = wei @ v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763f283b-0f80-4c64-9bc5-52868fd2a559",
   "metadata": {},
   "source": [
    "#### Output\n",
    "The output out[i] for the i-th token is a sum of all v[j] vectors, weighted by wei[i, j]. \\\n",
    "\n",
    "The final shape of out is (B, T, 16). Each of the 8 tokens in our sequence now has a new vector of size 16 that is context-aware, built by aggregating information from itself and all the tokens that came before it. This is the output of one attention head."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82101177-f922-430c-8dbc-e6725593443f",
   "metadata": {},
   "source": [
    "#### Step 5: Scaled Dot-Product Attention\n",
    "As the head_size gets larger (e.g., from 16 to 64 to 512), the dot product values will also get much larger and more spread out. \\\n",
    "Consider softmax([1, 2, 3]) -> [0.09, 0.24, 0.67]. The outputs are reasonably distributed. \\\n",
    "Now, imagine the head_size is large, and our dot products are bigger. Let's just multiply the inputs by 8: \\\n",
    "softmax([8, 16, 24]) -> [9.3e-08, 2.0e-04, 9.9e-01].\n",
    "\n",
    "Notice what happened? The output became extremely \"spiky\" or \"hard.\" It's practically [0, 0, 1]. \\\n",
    "The softmax is now super confident that the last element is the only one that matters.\n",
    "\n",
    "##### Summary:\n",
    "To keep the inputs to the softmax function under control, preventing them from becoming too large. This ensures that the gradients are stable and don't vanish, which allows the model to learn effectively, especially when using a large head_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc3ea29-4411-4192-b044-8c0b7eb38fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d817af-638b-466e-9630-03cf5109de7a",
   "metadata": {},
   "source": [
    "### All single head self attention code at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ecf7138b-69a9-4e1b-99ef-1ca6918e6965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All single head attetion code in one block\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)  # (B, T, 16)\n",
    "q = query(x)  # (B, T, 16)\n",
    "wei = (\n",
    "    q @ k.transpose(-2, -1) * head_size**-0.5\n",
    ")  # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "# wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float(\"-inf\"))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "# out = wei @ x\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "2921ff9f-a4c7-44b6-a02c-b854886b7a75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2853, 0.7147, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2858, 0.3704, 0.3437, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2679, 0.3740, 0.2292, 0.1289, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1926, 0.1797, 0.1312, 0.1444, 0.3521, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1533, 0.1322, 0.2156, 0.2225, 0.0987, 0.1777, 0.0000, 0.0000],\n",
       "        [0.0863, 0.1575, 0.1328, 0.1596, 0.1788, 0.1199, 0.1652, 0.0000],\n",
       "        [0.1044, 0.1764, 0.1101, 0.0950, 0.1406, 0.1058, 0.1436, 0.1241]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d0311c-1884-42b7-ad39-9f39289cf953",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb5929c8-35c3-4a33-8283-e57e49025655",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Linting check & format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "de437112-7b55-4742-b59f-0949a018aa13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All checks passed!\n",
      "5 files left unchanged\n"
     ]
    }
   ],
   "source": [
    "!uv run ruff check\n",
    "!uv run ruff format"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
